{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "actual-caribbean",
   "metadata": {
    "id": "actual-caribbean"
   },
   "source": [
    "<p style=\"text-align:center\"><font size=\"10\" >Kawase Seminar 03-1</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-infrastructure",
   "metadata": {
    "id": "distributed-infrastructure"
   },
   "source": [
    "# **スクレイピング入門**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-portable",
   "metadata": {
    "id": "productive-portable"
   },
   "source": [
    "03-1で知識を学び、03-2で実践にはいっていきます!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-mumbai",
   "metadata": {
    "id": "certain-mumbai"
   },
   "source": [
    "**スライドで簡単なスクレイピングの振り返り**\n",
    "https://docs.google.com/presentation/d/1jvaBd_rLNTmrY2NVpBajqVYrQKHr_bqSktMTsXcOZFY/edit#slide=id.gd91e1f37e_1_106"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-caribbean",
   "metadata": {
    "id": "incoming-caribbean"
   },
   "source": [
    "# **1.webクローリングとスクレイピングの概要**\n",
    "さて、いよいよwebスクレイピングを行っていきます! ここでは、まずwebスクレイピングの基礎知識であるクローリングとwebスクレイピングについてみていきます。\n",
    "その前に、前提知識としてwebについて軽く解説します。\n",
    "webから情報を取得するにはそもそもwebについて知らないと情報の収集をどのようにするか理解が難しいからです。\n",
    "\n",
    "[![Image from Gyazo](https://i.gyazo.com/4d801c5fb5c9aefed38dd2023bd8f68a.png)](https://gyazo.com/4d801c5fb5c9aefed38dd2023bd8f68a)\n",
    "\n",
    "引用: Pythonで始めるスクレイピング (Sansan DSOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-angle",
   "metadata": {
    "id": "endangered-angle"
   },
   "source": [
    "## **1.0現在のWebシステムについてざっくり解説**\n",
    "\n",
    "現在のWebシステムはwebクライアントとWebサーバに分かれてサービスを提供しています。\n",
    "\n",
    "[![Image from Gyazo](https://i.gyazo.com/334815c3f011505a1939f02cf451e378.png)](https://gyazo.com/334815c3f011505a1939f02cf451e378)\n",
    "\n",
    "引用: 小森 裕介 「プロになるためのWeb技術入門」 ――なぜ、あなたはWebシステムを開発できないのか\n",
    "\n",
    "\n",
    "尚、クライアント側からサーバに対する要求を「リクエスト」サーバからクライアントへの応答を「レスポンス」と言います\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-draft",
   "metadata": {
    "id": "gorgeous-draft"
   },
   "source": [
    "webシステムはHTML(Hyper Text Markup Language)というネットワーク経由で情報を閲覧するために作られたテキストファイルを使用しています。\n",
    "\n",
    "#### 【ざっくりHTML解説】\n",
    "><某>～</某> で一つの**「要素」**と言われる。  \n",
    ">下の 緑 の部分は**「要素名」**  \n",
    ">下の 黒 の部分は**「要素値」**  \n",
    ">下の 青 の部分は**「属性名」**  \n",
    ">下の 赤 の部分は**「属性値」**  \n",
    ">基本的に事前情報は `head` タグの内部に  \n",
    ">サイトの描画情報は `body` タグの内部に置かれる。  \n",
    ">`html` タグの内部が html 文章であることを示している。  \n",
    ">`meta` タグは検索エンジン最適化(SEO)のために使われるものです。  \n",
    ">`link` タグは CSS などの外部データを読み込む際に使われるものです。  \n",
    ">`title` タグはその名の通り文章のタイトルを示します。  \n",
    ">`h1`～`h6` タグは見出しを記述します。  \n",
    ">他に `a` `img` `ul` `ol` `li` `div` `span` `section` `blockquote` `main` `article` `form` `input` `textarea` `select` `option` `button` `script` などなどタグの種類はたくさんあります。  \n",
    ">属性名にも `class` `content` `href` `name` `id` 等あり、これらを使用してサイトは作られています。\n",
    "\n",
    "**RESASの例**\n",
    "~~~html\n",
    "<!doctype html>\n",
    "<html lang=\"ja\" class=\"rn\">\n",
    "    \n",
    "<head prefix=\"og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# article: http://ogp.me/ns/article# website: http://ogp.me/ns/website#\">\n",
    "  <meta charset=\"UTF-8\">\n",
    "\t<meta name=\"viewport\" content=\"width=1280\">\n",
    "  <meta name=\"description\" content=\"地域経済分析システム（RESAS：リーサス）は、地方自治体の様々な取り組みを情報面から支援するために、まち・ひと・しごと創生本部事務局が提供する、産業構造や人口動態、人の流れなどの官民ビッグデータを集約し、可視化するシステムです。\">\n",
    "  <title>トップページ - RESAS 地域経済分析システム</title>\n",
    "  <link rel=\"stylesheet\" href=\"./stylesheets/all.css\"/>\n",
    "  <link rel=\"shortcut icon\" href=\"./favicon.ico\"/>\n",
    "\n",
    "  <meta property=\"og:title\" content=\"トップページ - RESAS 地域経済分析システム\" />\n",
    "  <meta property=\"og:type\" content=\"website\" />\n",
    "  <meta property=\"og:url\" content=\"https://resas.go.jp/\" />\n",
    "  <meta property=\"og:image\" content=\"https://resas.go.jp/images/ogp/main.png\" />\n",
    "  <meta property=\"og:site_name\" content=\"RESAS 地域経済分析システム\" />\n",
    "  <meta property=\"og:description\" content=\"地域経済分析システム（RESAS：リーサス）は、地方自治体の様々な取り組みを情報面から支援するために、まち・ひと・しごと創生本部事務局が提供する、産業構造や人口動態、人の流れなどの官民ビッグデータを集約し、可視化するシステムです。\" />\n",
    "\n",
    "  <meta name=\"twitter:card\" content=\"summary_large_image\" />\n",
    "  <meta name=\"twitter:title\" content=\"トップページ - RESAS 地域経済分析システム\" />\n",
    "  <meta name=\"twitter:description\" content=\"地域経済分析システム（RESAS：リーサス）は、地方自治体の様々な取り組みを情報面から支援するために、まち・ひと・しごと創生本部事務局が提供する、産業構造や人口動態、人の流れなどの官民ビッグデータを集約し、可視化するシステムです。\" />\n",
    "  <meta name=\"twitter:image\" content=\"https://resas.go.jp/images/ogp/main.png\" />\n",
    "</head>\n",
    "    \n",
    "<body v-on=\"click: click($event)\" class=\"rn\">\n",
    "<div class=\"wrap\">\n",
    "  <main class=\"top\">\n",
    "    <div id=\"error-wrapper\"></div>\n",
    "    <div id=\"alert-wrapper\"></div>\n",
    "    <div id=\"loading-wrapper\"></div>\n",
    "    <div id=\"terminology-wrapper\" class=\"right-menu__popup\"></div>\n",
    "    <div id=\"header-wrapper\"></div>\n",
    "    <div id=\"main-wrapper\"></div>\n",
    "  </main>\n",
    "</div>\n",
    "\n",
    "<script src=\"./javascripts/lib.js\"></script>\n",
    "<script src=\"./javascripts/index.js\"></script>\n",
    "\n",
    "</body>\n",
    "\n",
    "</html>\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-buddy",
   "metadata": {
    "id": "prospective-buddy"
   },
   "source": [
    "### **URLについて :データのリソースの場所の特定**\n",
    "僕たちクライアントはサーバーからHTMLといったデータを取得します。その際の取得の元があるところがURL(Uniform Resource Locator)です。\n",
    "日本語で訳すとURLは「リソースの位置を指し示す統一的な記述方法」をといいます。\n",
    "\n",
    "[![Image from Gyazo](https://i.gyazo.com/ae4103888eeb1a45c12ff4689f9572ab.png)](https://gyazo.com/ae4103888eeb1a45c12ff4689f9572ab)\n",
    "引用: 小森 裕介 「プロになるためのWeb技術入門」 ――なぜ、あなたはWebシステムを開発できないのか\n",
    "\n",
    "\n",
    "URLにより、僕たちクライアントはサーバーの位置を特定して通信をしデータをやりとりします。\n",
    "\n",
    "では、その通信をするのはなんでしょうか。答えはHTTPです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-residence",
   "metadata": {
    "id": "productive-residence"
   },
   "source": [
    "### **HTTPとは**\n",
    "\n",
    "そもそもHTTPとは通信をするための取り決めのことです。\n",
    "\n",
    "例えば、昔の時代に流行った狼煙をイメージしましょう。\n",
    "狼煙は、敵がいた際に仲間に煙をあげることで警告を知らせるものです。この時、\n",
    "- 狼煙は敵がいることの合図である\n",
    "- 敵がいたら狼煙をあげて仲間に知らせる\n",
    "\n",
    "という意思疎通のルールがあることで成り立っています。\n",
    "\n",
    "HTTPはまさにネットワークにおける通信ルールのことを指します。(通信ルールであって技術自体を指さない ->技術実装に関してはportを利用)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-truck",
   "metadata": {
    "id": "employed-truck"
   },
   "source": [
    "### webシステムのまとめ\n",
    "\n",
    "つまりWebシステムとは、HTTPによってクライアントがリクエストを送り、サーバーがレスポンスを返すことの繰り返しにより成り立っています。\n",
    "\n",
    "[![Image from Gyazo](https://i.gyazo.com/f9462ac89880701cac3a47b0ad11ae95.png)](https://gyazo.com/f9462ac89880701cac3a47b0ad11ae95)\n",
    "\n",
    "引用: 小森 裕介 「プロになるためのWeb技術入門」 ――なぜ、あなたはWebシステムを開発できないのか"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-shark",
   "metadata": {
    "id": "ready-shark"
   },
   "source": [
    "### コラム：HTTPSとは\n",
    "\n",
    "近年ではHTTP通信だけではセキュリティに問題があるとされており、よりセキュアなHTTPS通信を用いてやりとりしています。\n",
    "\n",
    "例) https://www.nikkei.com/  , https://densuke.work/  \n",
    "\n",
    "[![Image from Gyazo](https://i.gyazo.com/577319722ad44f4f03f069d1c1e9fae0.png)](https://gyazo.com/577319722ad44f4f03f069d1c1e9fae0)\n",
    "引用: [津郷 晶也  「REST WebAPI サービス 設計」](https://www.udemy.com/course/rest-webapi-development/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-campaign",
   "metadata": {
    "id": "future-campaign"
   },
   "source": [
    "## 1.1**クローリングとは**\n",
    "webサイトからHTMLを取得する(クロール)することをクローリングと言います。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-bolivia",
   "metadata": {
    "id": "challenging-bolivia"
   },
   "source": [
    "### **クローリングの方法(HTMLの取得): requestsの利用**\n",
    "クローリングをする際に、webページにあるHTMLはrequestsを使用しHTTPリクエストを投げることで取得することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-green",
   "metadata": {
    "id": "sublime-green"
   },
   "outputs": [],
   "source": [
    "#クロールするためにHTTPライブラリ であるrequestsを利用\n",
    "import requests as req\n",
    "\n",
    "#getで呼び出している\n",
    "r = req.get(\"https://www.nikkei.com/\")\n",
    "# htmlの取得\n",
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-sheet",
   "metadata": {
    "id": "swedish-sheet"
   },
   "source": [
    "### 追加情報: クライアントからサーバーにリクエストを送る４つの方法\n",
    "\n",
    "[![Image from Gyazo](https://i.gyazo.com/f30e106c9e288c8634f0c1086e849836.png)](https://gyazo.com/f30e106c9e288c8634f0c1086e849836)\n",
    "\n",
    "引用: 津郷 晶也  「REST WebAPI サービス 設計」\n",
    "\n",
    "基本はgetメソッドだけ知っていれば大丈夫です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-latin",
   "metadata": {
    "id": "relevant-latin"
   },
   "source": [
    "### コラム: HTTPメソッドの利用例(API)\n",
    "\n",
    "[![Image from Gyazo](https://i.gyazo.com/6dd922eb22bece51a8ac9021ee2f5901.png)](https://gyazo.com/6dd922eb22bece51a8ac9021ee2f5901)\n",
    "\n",
    "引用: 津郷 晶也  「REST WebAPI サービス 設計」"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-pasta",
   "metadata": {
    "id": "parliamentary-pasta"
   },
   "source": [
    "---\n",
    "---\n",
    "## **1.2スクレイピングとは**\n",
    "クロールしたHTMLに記載している情報を抽出することをスクレイピングと言います。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-salmon",
   "metadata": {
    "id": "oriental-salmon"
   },
   "source": [
    "### **スクレイピングの方法: BeautifulSoapの利用**\n",
    "スクレイピングは、主にBeautifulSoupというHTMLやXMLから情報を抽出するためのライブラリが用いられます。\n",
    "\n",
    "Beautifulsoapライブラリについての使い方はこちら!\n",
    "\n",
    "引用:[AI-interのPython3入門 「図解！Python BeautifulSoupの使い方を徹底解説！(select、find、find_all、インストール、スクレイピングなど)」](https://ai-inter1.com/beautifulsoup_1/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "immune-patio",
   "metadata": {
    "id": "immune-patio",
    "outputId": "d4383a82-2460-4617-a493-9c80c4aa6676"
   },
   "outputs": [],
   "source": [
    "# Beautifulsoupと lxmlのインストール\n",
    "!pip install bs4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "earned-pakistan",
   "metadata": {
    "id": "earned-pakistan",
    "outputId": "2bff1f19-4fc2-40dc-c26e-94017b93b86f"
   },
   "outputs": [],
   "source": [
    "#クロールするためにHTTPライブラリ であるrequestsを利用\n",
    "import requests as req\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#getで呼び出している\n",
    "r = req.get(\"https://www.nikkei.com/\")\n",
    "\n",
    "#BeautifulSoupインスタンスを生成\n",
    "soup = BeautifulSoup(r.content, \"lxml\")\n",
    "\n",
    "#HTMLのtitleタグをfindする\n",
    "soup.find(\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "serial-torture",
   "metadata": {
    "id": "serial-torture",
    "outputId": "7a09c31e-1936-47d5-8faf-003b1e3a162e"
   },
   "outputs": [],
   "source": [
    "#cssセレクタを使っても要素を抽出することができる\n",
    "soup.select(\"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-karaoke",
   "metadata": {
    "id": "veterinary-karaoke"
   },
   "source": [
    "### **スクレイピングで表の取得をしたい場合: pandasの利用**\n",
    "\n",
    "表のみを取得したい場合、pandasライブラリを使うことで簡単に取得することができます。\n",
    "\n",
    "(pandasに関しては第4回にて別途解説)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sealed-observer",
   "metadata": {
    "id": "sealed-observer",
    "outputId": "881abb83-5d93-44a0-b6b2-577cb227151e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#株式投資メモからページ内のTαble群を取得\n",
    "dfs =  pd.read_html(\"https://kabuoji3.com/ranking/?date=2021-03-15&type=1&market=3\")\n",
    "#DataFrame型を複数持つリストを返す\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-richards",
   "metadata": {
    "id": "underlying-richards"
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# **2.動的なページの対策**\n",
    "\n",
    "さて、webページをクローリングし特定の情報をスクレイピングし抽出する方法はわかりましたが、ページ遷移を自動化し遷移先のデータを取得するといった処理はどうすればいいのでしょうか？\n",
    "\n",
    "\n",
    "[![Image from Gyazo](https://i.gyazo.com/231d759eb75b299499f95722c251f34e.png)](https://gyazo.com/231d759eb75b299499f95722c251f34e)\n",
    "\n",
    "引用: [Django独学 WEBページをリンクでページ遷移させる](https://loosecarrot.com/2021/01/03/4775/)\n",
    "\n",
    "\n",
    "また、近年ではWebシステムのユーザビリティ向上のためにSPA(Single Page Application)と呼ばれる、\n",
    "\n",
    "単一のWebページでアプリケーションを構成する設計構造が取られるなどをし、\n",
    "\n",
    "JavaScriptを利用し大量のHTMLデータを一気に送らずに部分的に送る仕組みが先行してしています。\n",
    "\n",
    "[![Image from Gyazo](https://i.gyazo.com/539d98c8e8bcd3e003b94be99d885bd3.png)](https://gyazo.com/539d98c8e8bcd3e003b94be99d885bd3)\n",
    "\n",
    "引用: SPA（シングルページアプリ）\n",
    "\n",
    "そのため、こうした動的なページに対して単にrequestsライブラリでGETリクエストをしHTMLを取得するだけでは全てのHTMLを取得することができません。\n",
    "\n",
    "その場合はどのようにデータを取得すれば良いのでしょうか？\n",
    "\n",
    "\n",
    "今回はそうしたページ遷移といった動作の自動化や動的ページを取得する方法を紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DmyFjcjPztIX",
   "metadata": {
    "id": "DmyFjcjPztIX"
   },
   "source": [
    "### **自動操作＆動的なページの対処: Seleniumの利用**\n",
    "自動操作＆動的なページの対処をするために、Seleniumというライブラリが用いられます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "further-mineral",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "further-mineral",
    "outputId": "a48a26e7-a48a-4986-e1a9-32e65f705a2a"
   },
   "outputs": [],
   "source": [
    "### seleniumのインストール(google colab限定のインストール方法)\n",
    "# reference https://enjoy-a-lot.com/google-colaboratory-selenium/#toc1\n",
    "!pip install selenium\n",
    "\n",
    "!apt-get update\n",
    "!apt install chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "YzqbcTIl0lHx",
   "metadata": {
    "id": "YzqbcTIl0lHx"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# ChromeDriver起動\n",
    "def generate_dirver():\n",
    "  options = webdriver.ChromeOptions()\n",
    "  options.add_argument('--headless')\n",
    "  options.add_argument('--no-sandbox')\n",
    "  options.add_argument('--disable-dev-shm-usage')\n",
    "  driver = webdriver.Chrome('chromedriver',options=options)\n",
    "  return driver\n",
    "\n",
    "driver = generate_dirver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accessory-northwest",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "accessory-northwest",
    "outputId": "1f2d253b-b8fa-46ff-c897-fe7e0796d068"
   },
   "outputs": [],
   "source": [
    "## seleniumの利用し日経新聞にアクセス\n",
    "driver.get('https://www.nikkei.com/')\n",
    "fetched = driver.find_element_by_class_name(\"k-button__text\")\n",
    "#seleniumはセッションを先に取得し、プロパティを指定するとセッションを元にデータを返す\n",
    "print(fetched, fetched.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YkrSkUoX198T",
   "metadata": {
    "id": "YkrSkUoX198T"
   },
   "outputs": [],
   "source": [
    "#processをキルする。\n",
    "driver.close()\n",
    "\n",
    "#google colab上だと自動操作が見えづらい、、w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-recovery",
   "metadata": {
    "id": "variable-recovery"
   },
   "source": [
    "[![Image from Gyazo](https://i.gyazo.com/d301757aedffa5e5b4e25865cf56cb56.png)](https://gyazo.com/d301757aedffa5e5b4e25865cf56cb56)\n",
    "引用:[AI-interのPython3入門 「図解！Python BeautifulSoupの使い方を徹底解説！(select、find、find_all、インストール、スクレイピングなど)」](https://ai-inter1.com/beautifulsoup_1/)\n",
    "\n",
    "Seleniumは、データ取得だけでなく、データの抽出も行うことができますが、ブラウザを操作してデータを取得しますので、動作が遅いことが難点です。\n",
    "\n",
    " \n",
    "従って、できるだけRequestsやBeautiful Soupを使い、Seleniumは必要最低限の箇所で使うことをお勧めします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-glucose",
   "metadata": {
    "id": "incorrect-glucose"
   },
   "source": [
    "---\n",
    "# **3.そのほかの対応**\n",
    "\n",
    "他にもスクレピングをする際に、ログイン認証が求められたりする場合や、定期的にデータ取得を自動で行いたい場合などがあります。\n",
    "\n",
    "一気に対策を学習するのは膨大なのでその都度対策を考えることをオススメします。\n",
    "\n",
    "[![Image from Gyazo](https://i.gyazo.com/4c5c99b15a12b947004d63e3b0ee9572.png)](https://gyazo.com/4c5c99b15a12b947004d63e3b0ee9572)\n",
    "引用: Pythonで始めるスクレイピング (Sansan DSOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-coordination",
   "metadata": {
    "id": "demographic-coordination"
   },
   "source": [
    "---\n",
    "# **4.スクレイピングをやる上での守らないといけないルール**\n",
    "\n",
    "- 事前にサイトの利用規約を確認して可能なサイトのスクレイピングを行いましょう。\n",
    "    - スクレイピング・クローリングに関しては各Web サイトの利用規約によって禁止しているところもあります。\n",
    "  \n",
    "    \n",
    "- できる限りAPI(Application Programming Interface)を利用しましょう。\n",
    "  - 大手 Web サイトでは情報収集においてスクレイピング・クローリングを禁止している代わりに、API(Application Programming Interface)を利用することを推奨しているものがあります。\n",
    "  - スクレイピング・クローリングが可能なサイトであっても常識的な範囲内での取得をお勧めします。\n",
    "\n",
    "\n",
    "- 一度情報を取得し、再度取得するまで最低でも 1 秒(理想は 3~5 秒)間隔をあけて処理を行うようにしましょう\n",
    "    - Web サイトはサーバー上にあり、情報それ自体もサーバー上にあります。\n",
    "    - サーバーに負荷をかけるようなプログラミングは故意でないとしても(時には良識的で礼儀正しい設計であっても)DoS(Denial of Service attack)攻撃とみなされて逮捕された例などがあるのでスクレイピング・クローリングを行う際はきちんと利用規約等を読みましょう。\n",
    "    - 実際の人間がネットサーフィンをする速度程度がサーバーに負荷をかけない処理速度と言えます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-merchandise",
   "metadata": {
    "id": "infrared-merchandise"
   },
   "source": [
    "## **4.1スクレイピングが問題となった事例**\n",
    "\n",
    ">2010年3月頃、市民から岡崎市立図書館のウェブサイトの蔵書検索システムに対し接続が出来ないと苦情があり、その後もウェブサイトの閲覧が困難になる事態が相次いだ。同年4月15日、同図書館が迷惑なアクセスを受けていると愛知県岡崎警察署に被害届を提出し、5月25日にアクセスを行っていた男性が蔵書検索システムに高頻度のリクエストを故意に送りつけたとして偽計業務妨害容疑で逮捕された。男性が実際に行っていたのは、蔵書検索システムの使い勝手に満足しなかったため自身で作成したクローラを実行し、蔵書検索システムから図書情報を取得することであった。クローラとは、自動的に情報を引き出しデータベースにまとめるプログラムであり、GoogleやYahoo!等の検索エンジンなどでも利用されている。また国立国会図書館でもインターネット資料収集のためクローラを用いている[1]。\n",
    "\n",
    "引用：[岡崎市立中央図書館事件](https://ja.wikipedia.org/wiki/%E5%B2%A1%E5%B4%8E%E5%B8%82%E7%AB%8B%E4%B8%AD%E5%A4%AE%E5%9B%B3%E6%9B%B8%E9%A4%A8%E4%BA%8B%E4%BB%B6\n",
    ")\n",
    "\n",
    "\n",
    "### 違法にならない対策を読んで理解しよう\n",
    "[スクレイピングは違法？3つの法律問題と対応策を弁護士が5分で解説](https://topcourt-law.com/internet_security/scraping-illegal#i-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-tobago",
   "metadata": {
    "id": "verbal-tobago"
   },
   "source": [
    "## 4.2 **意識しようスクレイピングのルール**\n",
    "\n",
    "### 1.Webページ上の情報を抜き出して再利用することを利用規約で禁じているサイトもある\n",
    "###   　　 → 必ずWebページ上の情報の著作権や利用規約を確認!!!\n",
    "\n",
    "### 2.スクレイピングは初学者にとってはむしろ危険\n",
    "###    　　→ 一般的にはサイトが提供しているAPIを使おう！\n",
    "\n",
    "### 3.必ずtime.sleepを用いること！ 1秒以上は間隔をあけること\n",
    "### 　　　→ 一歩間違えると犯罪になることを意識(Dos攻撃に間違われる)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-powder",
   "metadata": {
    "id": "distributed-powder"
   },
   "source": [
    "---\n",
    "# 5.appendix: オススメ書籍\n",
    "\n",
    "[Pythonクローリング＆スクレイピング［増補改訂版］\n",
    "―データ収集・解析のための実践開発ガイドー](https://gihyo.jp/book/2019/978-4-297-10738-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-description",
   "metadata": {
    "id": "variable-description"
   },
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "それではベースとなる知識を習得したところで実践に写っていきます！！！！"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "KawaseSeminar03-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
