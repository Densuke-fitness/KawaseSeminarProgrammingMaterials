{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "partial-eligibility",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\"><font size=\"10\" >Kawase Seminar 04-3\n",
    "    </font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-commerce",
   "metadata": {},
   "source": [
    "# **テキストマイニング入門**\n",
    "04-1ではテキストマイニングの基礎となるデータ分析の説明していきます。\n",
    "\n",
    "04-2ではPythonでデータ分析をするための基本操作(pandas)の勉強をしていきます。\n",
    "\n",
    "04-3 ではテキストマイニングの実践として、歌詞メーカーを実装していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-spider",
   "metadata": {},
   "source": [
    "### テキストマイニングの流れ\n",
    "1. テキストの読み込み\n",
    "\n",
    "2. 任意の方法で文字列を分割\n",
    "\n",
    "3. 文字列を数値化などし分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-cambodia",
   "metadata": {},
   "source": [
    "# 0. データセットのインストール\n",
    "\n",
    "テキストマイニングをしたいデータをセットします。\n",
    "git cloneで今回読み込むデータをインストールします。\n",
    "\n",
    "(データフォルダの中の「画像」フォルダには画像ファイル、「歌詞」のフォルダにいき分析した文字列が入ったtxtファイルをセットします)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-traffic",
   "metadata": {},
   "source": [
    "!git clone https://github.com/Densuke-fitness/pythonTextmining.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-madagascar",
   "metadata": {},
   "source": [
    "<h1>1. ライブラリのインポート</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install janome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストマイニングで使うライブラリの数々\n",
    "from gensim.models import word2vec\n",
    "import re\n",
    "import matplotlib as mplt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "# こちらは pip を使用\n",
    "from janome.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-hindu",
   "metadata": {},
   "source": [
    "<h1>2. 関数の設定</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 記号かどうかを判別する関数\n",
    "def find_kigou(string):\n",
    "    for s in string:\n",
    "        if s in [\"!\",\"?\",\"[\",\"]\",\"(\",\")\",\"「\",\"」\",\"！\",\"？\",\"（\",\"）\",\"'\",'\"',\",\",\".\"]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一文字だけの言葉を削除関数\n",
    "def token(words,tokens,h):\n",
    "    m = [chr(i) for i in range(12354,12354 + 82)]\n",
    "    for token in tokens:\n",
    "        word = token.surface\n",
    "        hinsi = token.part_of_speech.split(',')[0]\n",
    "        if hinsi == h:\n",
    "            key = word\n",
    "            if key not in words:\n",
    "                if find_kigou(key):\n",
    "                    # 一文字ひらがなの場合を削除\n",
    "                    if len(key) == 1 and key not in m:\n",
    "                        words[key] = 1\n",
    "                    # 二文字ひらがなの場合を削除\n",
    "                    if len(key) == 2 and (key[0] not in m and key[1] not in m):\n",
    "                        words[key] = 1\n",
    "                    # 三文字以上はすべて許可\n",
    "                    if len(key) >= 3 :\n",
    "                        words[key] = 1\n",
    "            else:\n",
    "                words[key] += 1\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 歌詞を読み込んで解析を行う関数\n",
    "def kashi(kashi_path,n):\n",
    "    #歌詞読み込み\n",
    "    rdata = open(f\"./pythonTextmining/データ/【歌詞】/{kashi_path}\",'r',encoding=\"utf-8_sig\")\n",
    "    data = rdata.read().replace('\\n', '')\n",
    "    #形態素解析\n",
    "    t = Tokenizer()\n",
    "    tokens = t.tokenize(data)\n",
    "    #名詞抽出\n",
    "    words = {}\n",
    "    words = token(words,tokens,\"名詞\")\n",
    "    if len(words) <= 50:\n",
    "        words = token(words,tokens,\"形容詞\")\n",
    "    if len(words) <= 50:\n",
    "        words = token(words,tokens,\"副詞\")\n",
    "        \n",
    "    #フォルダ作成        \n",
    "    kyokumei = kashi_path.split(\".\")[0]\n",
    "    os.mkdir(f\"./pythonTextmining/データ/【画像】/{kyokumei}\")\n",
    "    #テキストマイニング＆保存\n",
    "    for i in tqdm(range(n)):\n",
    "        wordcloud = WordCloud(font_path=\"HGRGM.TTC\",\n",
    "                              regexp=\"[\\w']+\",\n",
    "                              background_color='white',\n",
    "                              colormap='jet',\n",
    "                              width=800,\n",
    "                              height=800)\n",
    "        wordcloud.fit_words(words)\n",
    "#         plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "#         plt.show()\n",
    "        wordcloud.to_file(f'./pythonTextmining/データ/【画像】/{kyokumei}/{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストマイニングのファイル内に歌詞名のファイルを作成してその中に作成↑\n",
    "def make_txt_mining(n):\n",
    "    count = 0\n",
    "    kashi_list = os.listdir(\"./pythonTextmining/データ/【歌詞】\")\n",
    "    #for i in tqdm(range(len(kashi_list))):\n",
    "    for i in range(len(kashi_list)):\n",
    "        count += 1\n",
    "        ka = kashi_list[i].split(\".\")[0]\n",
    "        if not os.path.isdir(f\"./pythonTextmining/データ/【画像】/{ka}\"):\n",
    "            kashi(kashi_list[i],n)\n",
    "        if count < len(kashi_list):\n",
    "            clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kashi_mask(kashi_path,mask_path,n):\n",
    "    #歌詞読み込み\n",
    "    rdata = open(f\"./pythonTextmining/データ/【歌詞】/{kashi_path}\",'r',encoding=\"utf-8_sig\")\n",
    "    data = rdata.read().replace('\\n', '')\n",
    "    #マスク読み込み\n",
    "    msk = np.array(Image.open(f\"./pythonTextmining/データ/【mask】/{mask_path}\"))\n",
    "    #形態素解析\n",
    "    t = Tokenizer()\n",
    "    tokens = t.tokenize(data)\n",
    "    #名詞抽出\n",
    "    words = {}\n",
    "    words = token(words,tokens,\"名詞\")\n",
    "    if len(words) <= 30:\n",
    "        words = token(words,tokens,\"形容詞\")\n",
    "    if len(words) <= 30:\n",
    "        words = token(words,tokens,\"副詞\")\n",
    "    \n",
    "    #フォルダ作成        \n",
    "    kyokumei = kashi_path.split(\".\")[0]\n",
    "    kyokumei_mask = kyokumei + \"_mask\"\n",
    "    os.mkdir(f\"./pythonTextmining/データ/【画像】/{kyokumei_mask}\")\n",
    "    maskname = mask_path.split(\".\")[0]\n",
    "    #テキストマイニング＆保存\n",
    "    for i in tqdm(range(n)):\n",
    "        wordcloud = WordCloud(font_path=\"HGRGM.TTC\",\n",
    "                              font_step = 2, \n",
    "                              min_font_size = 1,\n",
    "                              regexp=\"[\\w']+\",\n",
    "                              #mode = \"RGBA\",\n",
    "                              background_color = \"aqua\",#背景の色\n",
    "                              mask=msk,\n",
    "                              contour_width=5,#輪郭線の太さ\n",
    "                              contour_color='yellow',#輪郭線の色\n",
    "                              width=800,\n",
    "                              height=800)\n",
    "        wordcloud.fit_words(words)\n",
    "        image_colors = ImageColorGenerator(msk)\n",
    "        wordcloud.recolor(color_func=image_colors)        #文字の色にmaskの色を反映させる場合はこれ\n",
    "        #plt.imshow(wordcloud,interpolation=\"bilinear\")   #maskの色を考慮しない場合、上の行は消してこれ　\n",
    "        plt.axis(\"off\")\n",
    "        #plt.show()\n",
    "        wordcloud.to_file(f\"./pythonTextmining/データ/【画像】/{kyokumei_mask}/{kyokumei}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-nevada",
   "metadata": {},
   "source": [
    "<h1>3. 関数呼び出し</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_txt_mining(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先ほど作成した関数を呼び出してあげる\n",
    "kashi_mask(\"お願いマッスル.txt\",\"マッチョ.jpg\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-happening",
   "metadata": {},
   "source": [
    "#  Reference\n",
    "今回は以前に田村が執筆した記事を参考に作成しています。興味がある方はぜひ見てください。\n",
    "\n",
    "- [Pythonでテキストマイニングをしよう！](https://defilabo.io/text_mining/#%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E3%83%9E%E3%82%A4%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%A8%E3%81%AF)\n",
    "\n",
    "- [Pythonによるデータ分析① タイタニックの生存予測をしよう！](https://defilabo.io/python_dataanalysis_01/#%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90%E3%81%A8%E3%81%AF)\n",
    "- [Pythonで機械学習入門①教師あり学習編](https://defilabo.io/python_machinelearning_01/)\n",
    "\n",
    "その他の記事はこちら\n",
    "- [田村の記事まとめサイト](https://densuke.work/Articles.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
