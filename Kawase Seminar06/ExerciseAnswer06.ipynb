{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExerciseAnswer05.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_ZSOhQTkjID"
      },
      "source": [
        "<p style=\"text-align:center\"><font size=\"10\" > Exercise Answer 06</font></p>\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEpHQ7TWkjIL"
      },
      "source": [
        "# Exercise 06-1 解答(あくまでも田村の回答)\n",
        "\n",
        "## 今回のテキストマイニングの部分\n",
        "\n",
        "### テキストマイニング部分①\n",
        "- メッセージデータから句読点や日本語以外の文字を削除してノイズを除去\n",
        "- 前処理データの一部を用いたSVM分類用のトレーニングデータの作成\n",
        "    - メッセージデータの中から1000個のメッセージをランダムに選択\n",
        "    - ないため、Antweiler and Frank (2004)やDas and Chen (2007)に倣って手動で強気、弱気、中立のメッセージに分類\n",
        "    - 次に、分類されたメッセージの中から、「売り」「買い」「高」など、強気・弱気の感情に大きく関係するキーワードを選択\n",
        "    -  SVMを用いて、メッセージデータに含まれるキーワードの組み合わせと頻度に基づいて、メッセージを強気、弱気、中立に分類する\n",
        "    \n",
        "### テキストマイニング部分②\n",
        "- 前処理されたすべてのメッセージはをMeCabを用いて解析しメッセージを言葉単位に分解\n",
        "- 言葉単位に分解したデータを元に、学習させたSVMを使いすべてのメッセージを3つのセンチメントクラスに分類\n",
        "\n",
        "\n",
        "\n",
        "## 課題や不確実な部分\n",
        "\n",
        "### テキストマイニング部分①\n",
        "1. 「第１段階、第２段階の各段階で収集したメッセージデータの中から1000個のメッセージをランダムに選択」とあるが、企業数分のデータを集めてランダムに選択するのは工数がかなりかかると予想。\n",
        "\n",
        "2. 1000個のメッセージを手動で強気、弱気、中立のに分類（将来の企業業績や市場の状況について楽観的な意見を表すメッセージは強気、悲観的な意見を表すメッセージは弱気に分類。また、将来の企業業績や市況について明確な感情がないメッセージは、中立的なものに分類）は工数がかなりかかると予想。\n",
        "\n",
        "3. 「フェーズ1では260個、フェーズ2では255個のキーワードを選択」の選択方法が選択者の主観に頼るため再現性が低いと予想。\n",
        "\n",
        "\n",
        "\n",
        "### テキストマイニング部分②\n",
        "\n",
        "4. 「情報やノイズの多い情報を除外するために167の単語を辞書に追加」とあるが詳細のデータを確認しないと再現性が低いと予想。\n",
        "\n",
        "5. すべてのメッセージをキーワード頻度行列に変換　--> 実装方法が不明。\n",
        "\n",
        "6. SVM利用し、3つのセンチメントクラスに分類　-> 5ができないと6を分類することが不可能。　\n",
        "\n",
        "    \n",
        "### 用語の確認\n",
        "- フェーズ１・・最初のメッセージが掲載されてからブックビルディングが終了するまでの期間\n",
        "- フェーズ２・・ブックビルディングが終了してからIPO直前までの期間\n",
        "    \n",
        " ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjuSXd8rkjIL"
      },
      "source": [
        "# Exercise 06-2 解答(あくまでも田村の回答)\n",
        "\n",
        "## 実装の手順(TODO)\n",
        "\n",
        "\n",
        "### テキストマイニング1の部分\n",
        "\n",
        "1. 第１段階、第２段階の各段階で収集したメッセージデータの中から1000個のメッセージをランダムに選択し、1つのdataframeに保存\n",
        "    1.  フォルダ内にある全てのcsvデータを読み込み1つのdataframeにまとめる\n",
        "    2. 読み込まれたdataframeのメッセージの項目から1000個抽出する\n",
        "　\n",
        "2. 1000個のメッセージを手動で強気、弱気、中立に分類\n",
        "\n",
        "3. ２で、分類されたメッセージの中から、「売り」「買い」「高」など、強気・弱気の感情に大きく関係するキーワードを選択。\n",
        "\n",
        "\n",
        "### テキストマイニング2の部分\n",
        "\n",
        "4. macabなどの形態素解析ライブラリでメッセージの単語を分割する(その際に情報やノイズの多い情報を除外するために167の単語を辞書に追加)\n",
        "\n",
        "5. 企業数分のメッセージ（形態素解析で分けられた単語群）配列をキーワード頻度行列に変換\n",
        "\n",
        "6. SVM利用し、3つのセンチメントクラスに分類"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG_62LVXLd97"
      },
      "source": [
        "---\n",
        "\n",
        "# Additional implementations: Tamura Challenge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjlsKf_6L9mR"
      },
      "source": [
        "##### 1: フォルダ内にある全てのcsvデータを読み込み\n",
        "\n",
        "\n",
        "# 読み込み\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# パスで指定したファイルの一覧をリスト形式で取得. （ここでは一階層下のtestファイル以下）\n",
        "csv_files = glob.glob('/content/TODO/Kawase Seminar06/todo-data/*.csv')\n",
        "\n",
        "#読み込むファイルのリストを表示\n",
        "for a in csv_files:\n",
        "    print(a)\n",
        "\n",
        "# #csvファイルの中身を追加していくリストを用意\n",
        "data_list = []\n",
        "\n",
        "#読み込むファイルのリストを走査\n",
        "for file in csv_files:\n",
        "    data_list.append(pd.read_csv(file))\n",
        "\n",
        "#リストを全て行方向に結合\n",
        "#axis=0:行方向に結合, sort\n",
        "df = pd.concat(data_list, axis=0, sort=True)\n",
        "df = df.reset_index(drop = True)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr6OUD6YL9vJ"
      },
      "source": [
        "##### 2: 読み込まれたdataframeのメッセージの項目から1000個抽出する ######\n",
        "\n",
        "# 今回は100個取得\n",
        "df_sample = df.sample(n=100)\n",
        "print(len(set(df_sample.index.values)), len(df_sample.index.values))\n",
        "df_sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI3dCUwXMAzX"
      },
      "source": [
        "### 答えのアンチパターン\n",
        "そもそも実装時のTODOがミスってるバージョン。ランダムサンプリングの実装部分がミスる。\n",
        "\n",
        "1. 第１段階、第２段階の各段階で収集したメッセージデータの中から1000個のメッセージをランダムに選択し、1つのdataframeに保存\n",
        "  1. ランダムに企業コードを選択するプログラムを実行し、選択された企業名からcsvデータを読み込む\n",
        "  2. 読み込まれたcsvデータのメッセージの項目から適当に1つ選択する\n",
        "  3. 1, 2を1000回繰り返しデータを保存する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSZgx8ImLd99"
      },
      "source": [
        "##### 1ランダムに企業コードを選択するプログラムを実行し、選択された企業名からcsvデータを読み込む ######\n",
        "import random as rd\n",
        "import pandas as pd\n",
        "\n",
        "# https://qiita.com/john-rocky/items/32909820f99486afee07 を参照\n",
        "def extract_file_from_dir(DIR: str) -> pd.DataFrame:\n",
        "    existing_files = os.listdir(DIR)\n",
        "    extracted_file = rd.choice(existing_files)\n",
        "    file_path = DIR + \"/\" + extracted_file\n",
        "    # print デバックでランダムで選択していることを確認\n",
        "    print(f\"file_path: {file_path}\")\n",
        "    df = pd.read_csv(file_path, index_col=0)\n",
        "    return df \n",
        "df = extract_file_from_dir(DIR)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF9ne6k8Ld9_"
      },
      "source": [
        "##### 2: 2読み込まれたcsvデータのメッセージの項目から適当に1つ選択する ######\n",
        "def extract_message_from_df(df:  pd.DataFrame) -> str :\n",
        "    messages = list(df[\"Text\"].values)\n",
        "    extracted_message = rd.choice(messages)\n",
        "    return extracted_message\n",
        "\n",
        "extracted_message = extract_message_from_df(df)\n",
        "extracted_message"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8cn8L79Ld-A"
      },
      "source": [
        "##### 3: 1, 2を1000回繰り返す #####\n",
        "DIR = \"/content/TODO/Kawase Seminar06/todo-data\"\n",
        "\n",
        "extracted_messages = []\n",
        "\n",
        "for i in range(1000):\n",
        "    df = extract_file_from_dir(DIR)\n",
        "    extracted_message = extract_message_from_df(df)\n",
        "    extracted_messages.append(extracted_message)\n",
        "\n",
        "extracted_messages"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}