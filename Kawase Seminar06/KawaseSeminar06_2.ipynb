{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYbVVCIy1Dda"
   },
   "source": [
    "<p style=\"text-align:center\"><font size=\"10\" >Kawase Seminar 06-2</font></p>\n",
    "\n",
    "全2回にわけて実際の論文の手法を実践し、スクレイピングとテキストマイングの利用を体験していきます。\n",
    "\n",
    "第6回でも、1.実際に現状を把握する, 2.解くための筋道を立てる, 3.Densuke(田村)が実装したコードを読んで2の筋道が実際に正しかったのか確認する, といったことを逐次行っていきます。\n",
    "\n",
    "今回以降は、時間の関係上実装はそこまで行えませんがかなりの学びになるので、皆さんも各自でトライして頂けると良いと思われます💪\n",
    "\n",
    "### **やること**\n",
    "-  テキストマイニング分野における実装を大まかに考える\n",
    "-  大まかな実装を細かい単位に分解し具体的なロジックを組み立てる\n",
    "-  **ロジックをもとに実装する ->イマココ**\n",
    "\n",
    "\n",
    "今回のコードを読む目的としては、どのように田村が実装したのかを田村の思考を理解し、\n",
    "\n",
    "実際に自分で書く際の糧になってもらいことにあります。\n",
    "\n",
    "\n",
    "---\n",
    "## **田村のTODOメモ**\n",
    "\n",
    "1. 第１段階、第２段階の各段階で収集したメッセージデータの中から1000個のメッセージをランダムに選択\n",
    "    1.  ランダムに企業コードを選択するプログラムを実行し、選択された企業名からcsvデータを読み込む\n",
    "    2. 読み込まれたcsvデータのメッセージの項目から適当に1つ選択する\n",
    "    3. 1, 2を1000回繰り返す\n",
    "　\n",
    "2. 1000個のメッセージを手動で強気、弱気、中立に分類\n",
    "\n",
    "3. ２で、分類されたメッセージの中から、「売り」「買い」「高」など、強気・弱気の感情に大きく関係するキーワードを選択。\n",
    "\n",
    "4. macabなどの形態素解析ライブラリでメッセージの単語を分割する(その際に情報やノイズの多い情報を除外するために167の単語を辞書に追加)\n",
    "\n",
    "5. 企業数分のメッセージ（形態素解析で分けられた単語群）配列をキーワード頻度行列に変換\n",
    "\n",
    "6. SVM利用し、3つのセンチメントクラスに分類\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "## 実装が可能な部分を踏まえたTODO\n",
    "\n",
    "今回、時間内や調査の関係上実装が難しいところがあるため、内容を変更してTODOを再作成しました。\n",
    "\n",
    "1. 企業のcsvデータから、macabなどの形態素解析ライブラリでメッセージの単語を分割\n",
    "2. メッセージをplus(1>=N),  minus(N<= -1 ), normal(±0) に分類\n",
    "3.  plus, minus, normalに分けたメッセージにある単語を多い順にソートする\n",
    "\n",
    "✴︎今回はSVMで学習させて分類するのではなく、すでに学習されたモデル(日本語評価極性辞書)を利用し分類\n",
    "https://www.cl.ecei.tohoku.ac.jp/Open_Resources-Japanese_Sentiment_Polarity_Dictionary.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. 企業のcsvデータから、macabなどの形態素解析ライブラリでメッセージの単語を分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 形態素解析とは、自然言語処理（NLP）の一部で、自然言語で書かれた文を言語上で意味を持つ最小単位(＝形態素)に分け、それぞれの品詞や変化などを判別することです。\n",
    "例えば　「庭には二羽ニワトリがいる」という文章を庭（名詞）/に（助詞）/は（助詞）/二（数詞）/羽（助数詞）/ニワトリ（名詞）/が（助詞）/いる（動詞）のように形態素に分解し、意味を割り出します。\n",
    "https://ledge.ai/morpho_analysis_japan/\n",
    "\n",
    "\n",
    "今回は形態素解析の方法の一つとしてオープンソースのmecabを利用\n",
    "https://qiita.com/taroc/items/b9afd914432da08dafc8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mecab\n",
      "  Using cached mecab-0.996.3.tar.gz (62 kB)\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /opt/conda/bin/python3.8 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-ni9z0ssd/mecab_6d5a05e438ea4c86b364223e7a4c16cf/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-ni9z0ssd/mecab_6d5a05e438ea4c86b364223e7a4c16cf/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-8w3ru3jr\n",
      "         cwd: /tmp/pip-install-ni9z0ssd/mecab_6d5a05e438ea4c86b364223e7a4c16cf/\n",
      "    Complete output (10 lines):\n",
      "    /bin/sh: 1: mecab-config: not found\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-install-ni9z0ssd/mecab_6d5a05e438ea4c86b364223e7a4c16cf/setup.py\", line 53, in <module>\n",
      "        include_dirs=cmd2(\"mecab-config --inc-dir\"),\n",
      "      File \"/tmp/pip-install-ni9z0ssd/mecab_6d5a05e438ea4c86b364223e7a4c16cf/setup.py\", line 19, in cmd2\n",
      "        return cmd1(strings).split()\n",
      "      File \"/tmp/pip-install-ni9z0ssd/mecab_6d5a05e438ea4c86b364223e7a4c16cf/setup.py\", line 15, in cmd1\n",
      "        return os.popen(strings).readlines()[0].rstrip()\n",
      "    IndexError: list index out of range\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/b8/60/7059e1f60969544c45b2c3edacffe1b07932db9ee623a646d06d2173568d/mecab-0.996.3.tar.gz#sha256=0a943743149a00f8ff616c05cdb267618657a48e37b1381c5f8b41e9af8f32ad (from https://pypi.org/simple/mecab/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "  Using cached mecab-0.996.2.tar.gz (62 kB)\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /opt/conda/bin/python3.8 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-ni9z0ssd/mecab_1557faabb5ba4087acbfb3f16815135f/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-ni9z0ssd/mecab_1557faabb5ba4087acbfb3f16815135f/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-tb05xbxh\n",
      "         cwd: /tmp/pip-install-ni9z0ssd/mecab_1557faabb5ba4087acbfb3f16815135f/\n",
      "    Complete output (10 lines):\n",
      "    /bin/sh: 1: mecab-config: not found\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-install-ni9z0ssd/mecab_1557faabb5ba4087acbfb3f16815135f/setup.py\", line 53, in <module>\n",
      "        include_dirs=cmd2(\"mecab-config --inc-dir\"),\n",
      "      File \"/tmp/pip-install-ni9z0ssd/mecab_1557faabb5ba4087acbfb3f16815135f/setup.py\", line 19, in cmd2\n",
      "        return cmd1(strings).split()\n",
      "      File \"/tmp/pip-install-ni9z0ssd/mecab_1557faabb5ba4087acbfb3f16815135f/setup.py\", line 15, in cmd1\n",
      "        return os.popen(strings).readlines()[0].rstrip()\n",
      "    IndexError: list index out of range\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/0d/c5/3ac5ad489d296d048b9234d81062eed1b5249583f53a87955402b40516c2/mecab-0.996.2.tar.gz#sha256=ec8e46e4930e091c25f9f2dc740543bfc483482917dc1340fd914a9344de5b10 (from https://pypi.org/simple/mecab/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "  Using cached mecab-0.996.tar.gz (63 kB)\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /opt/conda/bin/python3.8 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-ni9z0ssd/mecab_7efaa713400a465f871f43abd69f8951/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-ni9z0ssd/mecab_7efaa713400a465f871f43abd69f8951/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-v5kd938b\n",
      "         cwd: /tmp/pip-install-ni9z0ssd/mecab_7efaa713400a465f871f43abd69f8951/\n",
      "    Complete output (10 lines):\n",
      "    /bin/sh: 1: mecab-config: not found\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-install-ni9z0ssd/mecab_7efaa713400a465f871f43abd69f8951/setup.py\", line 43, in <module>\n",
      "        include_dirs=cmd2(\"mecab-config --inc-dir\"),\n",
      "      File \"/tmp/pip-install-ni9z0ssd/mecab_7efaa713400a465f871f43abd69f8951/setup.py\", line 14, in cmd2\n",
      "        return cmd1(strings).split()\n",
      "      File \"/tmp/pip-install-ni9z0ssd/mecab_7efaa713400a465f871f43abd69f8951/setup.py\", line 10, in cmd1\n",
      "        return os.popen(strings).readlines()[0].rstrip()\n",
      "    IndexError: list index out of range\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/63/b2/a240ad681c8be36837be13d805ea8f31d94c3bf5949db5f68a2c46bb14f4/mecab-0.996.tar.gz#sha256=ae166290fb0a4e322f6692a02716528fe34b7812f9c36d6fcd453e50ce4b3833 (from https://pypi.org/simple/mecab/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement mecab\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for mecab\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "tagger = MeCab.Tagger(\"-Owakati\") \n",
    "text = \"この木なんの木気になる木\"\n",
    "print(tagger.parse(text).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**形態素解析をするメッセージデータのアップロード**\n",
    "\n",
    "今回は、前回取得したメッセージデータを採用。\n",
    "https://drive.google.com/file/d/1r4DWXDwqluxhxnnaFS9Pdoh5lNND0Ua-/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"東和ハイシステム(株).csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda x: tagger.parse(x).split()\n",
    "df[\"Text\"] = df[\"Text\"].map(func)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. メッセージをplus(1>=N), minus(N<= -1 ), normal(±0) に分類\n",
    "\n",
    "今回は東北大学の乾・鈴木研究室のページで公開されている日本語評価極性辞書を使った\n",
    "\n",
    "Sentiment Analysis (いわゆるネガポジ判定) ライブラリ oseti を採用。\n",
    "https://qiita.com/yukinoi/items/46aa016d83bb0e64f598"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install oseti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oseti\n",
    "\n",
    "analyzer = oseti.Analyzer()\n",
    "\n",
    "analyzer.analyze('天国で待ってる。')\n",
    "\n",
    "analyzer.analyze('遅刻したけど楽しかったし嬉しかった。すごく充実した！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda x: analyzer.analyze(x)[0]\n",
    "df[\"Text_emotion_val\"] = df[\"Text\"].map(func)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. plus, minus, normalに分けたメッセージにある単語を多い順にソートする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plus = df[df['Text_emotion_val'] > 0]\n",
    "plus = plus.copy()\n",
    "normal = df[df['Text_emotion_val'] == 0]\n",
    "normal = normal.copy()\n",
    "minus = df[df['Text_emotion_val'] < 0]\n",
    "minus = minus.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "tagger = MeCab.Tagger(\"-Ochasen\") \n",
    "\n",
    "def convert_text_nouns(text) :\n",
    "    nouns = [line.split()[0] for line in tagger.parse(text).splitlines() if \"名詞\" in line.split()[-1]]\n",
    "    return nouns \n",
    "\n",
    "for data in [plus, normal, minus]:\n",
    "    data[\"splited_data\"] = data[\"Text\"].map(convert_text_nouns)\n",
    "    \n",
    "    word_counts = {} \n",
    "    for values in data[\"splited_data\"] :\n",
    "        for val in values :\n",
    "            if  val  in word_counts :\n",
    "                word_counts[val] += 1 \n",
    "            else:\n",
    "                word_counts[val] = 1 \n",
    "            \n",
    "    \n",
    "    score_sorted = sorted(word_counts.items(), key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    print(score_sorted)\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "- https://note.com/junmaeda/n/n39b8b286da13\n",
    "\n",
    "- https://qiita.com/g-k/items/1b7c765fa6520297ca7c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing remarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Image from Gyazo](https://i.gyazo.com/301c46663a66c700eea1aa066e4a0e2b.png)](https://gyazo.com/301c46663a66c700eea1aa066e4a0e2b)\n",
    "\n",
    "[![Image from Gyazo](https://i.gyazo.com/27cc5af78af3ef4d7a4244f493227c8d.png)](https://gyazo.com/27cc5af78af3ef4d7a4244f493227c8d)\n",
    "\n",
    "[![Image from Gyazo](https://i.gyazo.com/64f2c5739bbd4f6ac04c0b6e93ce3ee2.png)](https://gyazo.com/64f2c5739bbd4f6ac04c0b6e93ce3ee2)\n",
    "\n",
    "[![Image from Gyazo](https://i.gyazo.com/ee800b74e2f49d75013c18e0705ea149.png)](https://gyazo.com/ee800b74e2f49d75013c18e0705ea149)\n",
    "\n",
    "\n",
    "<h4 style=\"text-align:center;\"> みなさんも実際にプログラムを作成してみてください!!!! 良いデータサイエンスライフを!!!!💪 </h4>\n",
    "\n",
    "\n",
    "[![Image from Gyazo](https://i.gyazo.com/d9b57ca7fe3bc438fa9a7c60ca027940.png)](https://gyazo.com/d9b57ca7fe3bc438fa9a7c60ca027940)\n",
    "\n",
    "https://github.com/Densuke-fitness\n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional implementations: Tamura Challenge\n",
    "\n",
    "せっかくなので、手動でデータを分類することを見越して下記のTODOが実行可能な状態に持っていけるようにします。\n",
    "\n",
    "今回はディスカッションしながらゴールに辿りこうと思います。\n",
    "\n",
    "\n",
    "1. 第１段階、第２段階の各段階で収集したメッセージデータの中から1000個のメッセージをランダムに選択し、1つのdataframeに保存\n",
    "    1.  ランダムに企業コードを選択するプログラムを実行し、選択された企業名からcsvデータを読み込む\n",
    "    2. 読み込まれたcsvデータのメッセージの項目から適当に1つ選択する\n",
    "    3. 1, 2を1000回繰り返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "KawaseSeminar05-2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
